{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:39:47.222935Z",
     "iopub.status.busy": "2025-05-02T06:39:47.222627Z",
     "iopub.status.idle": "2025-05-02T06:39:47.228199Z",
     "shell.execute_reply": "2025-05-02T06:39:47.227389Z",
     "shell.execute_reply.started": "2025-05-02T06:39:47.222913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, logging\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "import nltk\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:25:46.369168Z",
     "iopub.status.busy": "2025-05-01T23:25:46.368630Z",
     "iopub.status.idle": "2025-05-01T23:25:46.372745Z",
     "shell.execute_reply": "2025-05-01T23:25:46.372078Z",
     "shell.execute_reply.started": "2025-05-01T23:25:46.369144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:25:47.923201Z",
     "iopub.status.busy": "2025-05-01T23:25:47.922840Z",
     "iopub.status.idle": "2025-05-01T23:25:47.927329Z",
     "shell.execute_reply": "2025-05-01T23:25:47.926721Z",
     "shell.execute_reply.started": "2025-05-01T23:25:47.923176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:37:53.525412Z",
     "iopub.status.busy": "2025-05-01T23:37:53.525131Z",
     "iopub.status.idle": "2025-05-01T23:37:53.531894Z",
     "shell.execute_reply": "2025-05-01T23:37:53.531112Z",
     "shell.execute_reply.started": "2025-05-01T23:37:53.525395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KeywordDataset(Dataset):\n",
    "    def __init__(self, abstracts, keywords, tokenizer, max_length=1024, max_target_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.abstracts = abstracts\n",
    "        self.keywords = keywords\n",
    "        self.max_length = max_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.abstracts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        abstract = \"Based on the following paper abstract, predict the keywords of the paper: \" + self.abstracts[idx]\n",
    "        \n",
    "        # Convert keyword list to a comma-separated string\n",
    "        keyword_list = self.keywords[idx]\n",
    "        if isinstance(keyword_list, list):\n",
    "            keywords_text = \", \".join(keyword_list)\n",
    "        else:\n",
    "            keywords_text = keyword_list\n",
    "        \n",
    "        # Encode the inputs\n",
    "        inputs = self.tokenizer(\n",
    "            abstract, \n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Encode the targets\n",
    "        targets = self.tokenizer(\n",
    "            keywords_text,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs.input_ids.squeeze()\n",
    "        attention_mask = inputs.attention_mask.squeeze()\n",
    "        labels = targets.input_ids.squeeze()\n",
    "        \n",
    "        # Replace padding token id with -100 so it's ignored in loss calculation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:37:56.295075Z",
     "iopub.status.busy": "2025-05-01T23:37:56.294767Z",
     "iopub.status.idle": "2025-05-01T23:37:56.300178Z",
     "shell.execute_reply": "2025-05-01T23:37:56.299522Z",
     "shell.execute_reply.started": "2025-05-01T23:37:56.295053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"Clean and prepare the dataframes for training\"\"\"\n",
    "    # Clean abstracts\n",
    "    for df in [train_df, test_df]:\n",
    "        df['abstract_content'] = df['abstract_content'].str.replace(\"\\n\\n\", \" \", regex=False)\n",
    "        df['abstract_content'] = df['abstract_content'].str.replace(\"\\n\", \" \", regex=False)\n",
    "        df['abstract_content'] = df['abstract_content'].str.strip()\n",
    "        \n",
    "        # Ensure keywords are in list format\n",
    "        if not isinstance(df['keywords'].iloc[0], list):\n",
    "            df['keywords'] = df['keywords'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "            \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:38:04.963243Z",
     "iopub.status.busy": "2025-05-01T23:38:04.962775Z",
     "iopub.status.idle": "2025-05-01T23:38:04.967599Z",
     "shell.execute_reply": "2025-05-01T23:38:04.966851Z",
     "shell.execute_reply.started": "2025-05-01T23:38:04.963219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_training_data(train_df, test_df, model_name=\"facebook/bart-base\"):\n",
    "    \"\"\"Prepare datasets for BART training\"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = KeywordDataset(\n",
    "        train_df['abstract_content'].tolist(),\n",
    "        train_df['keywords'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    eval_dataset = KeywordDataset(\n",
    "        test_df['abstract_content'].tolist(),\n",
    "        test_df['keywords'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    return train_dataset, eval_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:38:07.896235Z",
     "iopub.status.busy": "2025-05-01T23:38:07.895879Z",
     "iopub.status.idle": "2025-05-01T23:38:07.901467Z",
     "shell.execute_reply": "2025-05-01T23:38:07.900813Z",
     "shell.execute_reply.started": "2025-05-01T23:38:07.896213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_bart_model(train_dataset, eval_dataset, tokenizer, model_name=\"facebook/bart-base\"):\n",
    "    \"\"\"Train the BART model for keyword extraction\"\"\"\n",
    "    # Initialize model\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./bart-keyword-model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=10,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=128,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        gradient_accumulation_steps=4,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:25:45.715929Z",
     "iopub.status.busy": "2025-05-02T06:25:45.715678Z",
     "iopub.status.idle": "2025-05-02T06:25:45.721817Z",
     "shell.execute_reply": "2025-05-02T06:25:45.721031Z",
     "shell.execute_reply.started": "2025-05-02T06:25:45.715914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_fuzzy_map_score(true_keywords, predicted_keywords, fuzzy_threshold=80):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision with fuzzy matching\n",
    "    \"\"\"\n",
    "    if not predicted_keywords or not true_keywords:\n",
    "        return 0.0\n",
    "    \n",
    "    # Track which true keywords have been matched\n",
    "    matched_keywords = set()\n",
    "    precision_sum = 0.0\n",
    "    num_hits = 0\n",
    "    \n",
    "    # Check each predicted keyword in order\n",
    "    for i, pred_kw in enumerate(predicted_keywords):\n",
    "        # Try to match with any unmatched true keyword\n",
    "        for true_kw in true_keywords:\n",
    "            if true_kw in matched_keywords:\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity\n",
    "            similarity = fuzz.ratio(pred_kw.lower(), true_kw.lower())\n",
    "            \n",
    "            # If it's a match, count it and mark as matched\n",
    "            if similarity >= fuzzy_threshold:\n",
    "                matched_keywords.add(true_kw)\n",
    "                num_hits += 1\n",
    "                precision_sum += num_hits / (i + 1)  # Precision at recall point i+1\n",
    "                break\n",
    "    \n",
    "    # Calculate final MAP\n",
    "    if num_hits > 0:\n",
    "        return precision_sum / min(len(true_keywords), len(predicted_keywords))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:49:14.170573Z",
     "iopub.status.busy": "2025-05-02T06:49:14.169834Z",
     "iopub.status.idle": "2025-05-02T06:49:14.181102Z",
     "shell.execute_reply": "2025-05-02T06:49:14.180337Z",
     "shell.execute_reply.started": "2025-05-02T06:49:14.170535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_semantic_map_score(true_keywords, predicted_keywords, semantic_model, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision with semantic similarity matching\n",
    "    \"\"\"\n",
    "    if not predicted_keywords or not true_keywords:\n",
    "        return 0.0\n",
    "    \n",
    "    # Track which true keywords have been matched\n",
    "    matched_keywords = set()\n",
    "    precision_sum = 0.0\n",
    "    num_hits = 0\n",
    "\n",
    "    semantic_model.show_progress_bar = False\n",
    "\n",
    "    #Converting predicted and true keywords to embeddings\n",
    "    true_embeddings = semantic_model.encode(true_keywords, convert_to_tensor=True, show_progress_bar=False)\n",
    "    predicted_embeddings = semantic_model.encode(predicted_keywords, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    for i, pred_emb in enumerate(predicted_embeddings):\n",
    "        unmatched_indices = [j for j, kw in enumerate(true_keywords) if kw not in matched_keywords]\n",
    "        if not unmatched_indices:\n",
    "            break\n",
    "        \n",
    "        unmatched_true_embs = true_embeddings[unmatched_indices]\n",
    "        cos_scores = util.cos_sim(pred_emb, unmatched_true_embs)[0] #calculating cosine similarity to all unmatched keywords\n",
    "        \n",
    "        # Find best match\n",
    "        best_idx = torch.argmax(cos_scores).item()\n",
    "        best_score = cos_scores[best_idx].item()\n",
    "\n",
    "        if best_score >= similarity_threshold:\n",
    "            matched_kw = true_keywords[unmatched_indices[best_idx]]\n",
    "            matched_keywords.add(matched_kw)\n",
    "            num_hits += 1\n",
    "            precision_sum += num_hits / (i + 1)\n",
    "    \n",
    "    if num_hits > 0:\n",
    "        return precision_sum / min(len(true_keywords), len(predicted_keywords))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:49:21.860815Z",
     "iopub.status.busy": "2025-05-02T06:49:21.860531Z",
     "iopub.status.idle": "2025-05-02T06:49:21.868139Z",
     "shell.execute_reply": "2025-05-02T06:49:21.867501Z",
     "shell.execute_reply.started": "2025-05-02T06:49:21.860794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_keyword_extraction(model, tokenizer, test_df, fuzzy_threshold=50, semantic_threshold=0.5, top_k=5):\n",
    "    \"\"\"Evaluate the model using MAP with fuzzy matching\"\"\"\n",
    "    model.eval()\n",
    "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    fuzzy_map_scores = []\n",
    "    semantic_map_scores = []\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    for i, row in notebook_tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        abstract = row['abstract_content']\n",
    "        true_keywords = row['keywords']\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(abstract, return_tensors=\"pt\", max_length=1024, \n",
    "                          truncation=True).to(device)\n",
    "        \n",
    "        # Generate keywords\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids, \n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=5,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        # Decode and split the generated keywords\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted_keywords = [k.strip() for k in pred_text.split(',')][:top_k]\n",
    "        \n",
    "        # Calculate Fuzzy MAP score\n",
    "        fuzzy_map_score = calculate_fuzzy_map_score(true_keywords, predicted_keywords, fuzzy_threshold)\n",
    "        fuzzy_map_scores.append(fuzzy_map_score)\n",
    "\n",
    "        # Calculate Semantic MAP score\n",
    "        semantic_map_score = calculate_semantic_map_score(true_keywords, predicted_keywords, semantic_model, semantic_threshold)\n",
    "        semantic_map_scores.append(semantic_map_score)\n",
    "    \n",
    "    # Calculate average MAP scores\n",
    "    avg_fuzzy_map = sum(fuzzy_map_scores) / len(fuzzy_map_scores)\n",
    "    avg_semantic_map = sum(semantic_map_scores) / len(semantic_map_scores)\n",
    "    #print(f\"MAP@{top_k} Score with fuzzy threshold {fuzzy_threshold}: {avg_map:.4f}\")\n",
    "    \n",
    "    return avg_fuzzy_map, avg_semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:38:18.173752Z",
     "iopub.status.busy": "2025-05-01T23:38:18.173487Z",
     "iopub.status.idle": "2025-05-01T23:38:18.179993Z",
     "shell.execute_reply": "2025-05-01T23:38:18.179351Z",
     "shell.execute_reply.started": "2025-05-01T23:38:18.173733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def example_predictions(model, tokenizer, test_df, num_examples=5):\n",
    "    \"\"\"Show example predictions from the model\"\"\"\n",
    "    indices = np.random.choice(len(test_df), min(num_examples, len(test_df)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        abstract = test_df.iloc[idx]['abstract_content']\n",
    "        true_keywords = test_df.iloc[idx]['keywords']\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(abstract, return_tensors=\"pt\", max_length=1024, \n",
    "                          truncation=True).to(device)\n",
    "        \n",
    "        # Generate keywords\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids, \n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode and split the generated keywords\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted_keywords = [k.strip() for k in pred_text.split(',')]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ABSTRACT SNIPPET:\")\n",
    "        print(abstract[:300] + \"...\" if len(abstract) > 300 else abstract)\n",
    "        print(\"\\nTRUE KEYWORDS:\")\n",
    "        print(true_keywords)\n",
    "        print(\"\\nPREDICTED KEYWORDS:\")\n",
    "        print(predicted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:38:21.039998Z",
     "iopub.status.busy": "2025-05-01T23:38:21.039350Z",
     "iopub.status.idle": "2025-05-01T23:38:29.074610Z",
     "shell.execute_reply": "2025-05-01T23:38:29.074050Z",
     "shell.execute_reply.started": "2025-05-01T23:38:21.039976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(\"data/training-data.ndjson\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = [json.loads(line) for line in f if line.strip()]  # skip blank lines\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df = train_df[['abstract_content', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:38:37.990242Z",
     "iopub.status.busy": "2025-05-01T23:38:37.989929Z",
     "iopub.status.idle": "2025-05-01T23:38:40.744062Z",
     "shell.execute_reply": "2025-05-01T23:38:40.743496Z",
     "shell.execute_reply.started": "2025-05-01T23:38:37.990220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open(\"data/test-data.ndjson\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = [json.loads(line) for line in f if line.strip()]  # skip blank lines\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df = test_df[['abstract_content', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T23:38:48.395876Z",
     "iopub.status.busy": "2025-05-01T23:38:48.395223Z",
     "iopub.status.idle": "2025-05-02T06:03:13.755309Z",
     "shell.execute_reply": "2025-05-02T06:03:13.754547Z",
     "shell.execute_reply.started": "2025-05-01T23:38:48.395853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training loop for BART model trained on abstract contents\n",
      "Training on 16000 examples, testing on 4000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cdf9ad05cf453ba6298ff44a5695c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ced1fe29494e91a75c97b4a746cca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22ed109a4b24660a59f5d450652eb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5526cc1600c43979af9c7845838fcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a3ce0d98c147c0b8651da21b77b685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 6:08:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.816900</td>\n",
       "      <td>2.516139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.546400</td>\n",
       "      <td>2.425934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.340300</td>\n",
       "      <td>2.395354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.218800</td>\n",
       "      <td>2.362302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.056900</td>\n",
       "      <td>2.374098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.922900</td>\n",
       "      <td>2.365574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.884000</td>\n",
       "      <td>2.366636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.796600</td>\n",
       "      <td>2.383343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.756000</td>\n",
       "      <td>2.400268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.759300</td>\n",
       "      <td>2.397155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [16:04<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP score for model trained on abstracts: 0.40561243055555835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "train_df, test_df = preprocess_data(train_df, test_df)\n",
    "print(f\"Beginning training loop for BART model trained on abstract contents\")\n",
    "print(f\"Training on {len(train_df)} examples, testing on {len(test_df)} examples\")\n",
    "    \n",
    "train_dataset, eval_dataset, tokenizer = prepare_training_data(train_df, test_df)\n",
    "    \n",
    "#Model Training\n",
    "model, trainer = train_bart_model(train_dataset, eval_dataset, tokenizer)\n",
    "    \n",
    "#Model Evaluation\n",
    "fuzzy_map_score, semantic_map_score = evaluate_keyword_extraction(model, tokenizer, test_df)\n",
    "print(f\"Fuzzy Matching MAP score for model trained on abstracts: {fuzzy_map_score}\")\n",
    "print(f\"Semantic Matching MAP score for model trained on abstracts: {semantic_map_score}\")\n",
    "    \n",
    "#Visualizing Predictions\n",
    "#example_predictions(model, tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:49:27.299270Z",
     "iopub.status.busy": "2025-05-02T06:49:27.298581Z",
     "iopub.status.idle": "2025-05-02T07:07:16.411283Z",
     "shell.execute_reply": "2025-05-02T07:07:16.410500Z",
     "shell.execute_reply.started": "2025-05-02T06:49:27.299246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0237cfe43d475490a485043d6e5f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Matching MAP score for model trained on abstracts: 0.40561243055555835\n",
      "Semantic Matching MAP score for model trained on abstracts: 0.5819333333333369\n"
     ]
    }
   ],
   "source": [
    "fuzzy_map_score, semantic_map_score = evaluate_keyword_extraction(model, tokenizer, test_df)\n",
    "print(f\"Fuzzy Matching MAP score for model trained on abstracts: {fuzzy_map_score}\")\n",
    "print(f\"Semantic Matching MAP score for model trained on abstracts: {semantic_map_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:29:09.070662Z",
     "iopub.status.busy": "2025-05-02T07:29:09.069982Z",
     "iopub.status.idle": "2025-05-02T07:46:52.371629Z",
     "shell.execute_reply": "2025-05-02T07:46:52.370842Z",
     "shell.execute_reply.started": "2025-05-02T07:29:09.070640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e8832093e0448a9723a9c0750ab6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Matching MAP score for model trained on abstracts: 0.40561243055555835\n",
      "Semantic Matching MAP score for model trained on abstracts: 0.4505481944444467\n"
     ]
    }
   ],
   "source": [
    "fuzzy_map_score, semantic_map_score = evaluate_keyword_extraction(model, tokenizer, test_df, semantic_threshold=0.6)\n",
    "print(f\"Fuzzy Matching MAP score for model trained on abstracts: {fuzzy_map_score}\")\n",
    "print(f\"Semantic Matching MAP score for model trained on abstracts: {semantic_map_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:48:15.745226Z",
     "iopub.status.busy": "2025-05-02T07:48:15.744916Z",
     "iopub.status.idle": "2025-05-02T07:48:16.785463Z",
     "shell.execute_reply": "2025-05-02T07:48:16.784665Z",
     "shell.execute_reply.started": "2025-05-02T07:48:15.745204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ABSTRACT SNIPPET:\n",
      "Deep Convolutional Neural Networks (DCNNs) were originally inspired by principles of biological vision, have evolved into best current computational models of object recognition, and consequently indicate strong architectural and functional parallelism with the ventral visual pathway throughout comp...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['vision', 'attention', 'brain', 'deep neural networks', 'eye tracking', 'saliency map', 'object recognition', 'face detection']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['Deep Convolutional Neural Networks (DCNNs)', 'Attention Mechanism (ADM)', 'Human-like Attention']\n",
      "\n",
      "================================================================================\n",
      "ABSTRACT SNIPPET:\n",
      "State-of-the-art parametric and non-parametric style transfer approaches are prone to either distorted local style patterns due to global statistics alignment, or unpleasing artifacts resulting from patch mismatching. In this paper, we study a novel semi-parametric neural style transfer framework th...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['Neural style transfer · Graph neural networks · Attention-based message passing']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['— Semi-parametric', 'Graph Neural Network', 'Style Transfer1']\n",
      "\n",
      "================================================================================\n",
      "ABSTRACT SNIPPET:\n",
      "Speech coding facilitates the transmission of speech over low-bandwidth networks with minimal distortion. Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches. While this new generation of codecs is capable of synthesizing high...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "[': speech coding', 'Transformers', 'self-supervisedlearning', 'generative adversarial nets']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "[': speech coding', 'neural-network', 'transformer', 'low-bandwidth1']\n",
      "\n",
      "================================================================================\n",
      "ABSTRACT SNIPPET:\n",
      "The intelligent video surveillance system (IVSS) can automatically analyze the content of the surveillance image (SI) and reduce the burden of the manual labour. However, the SIs may suffer quality degradations in the procedure of acquisition, compression, and transmission, which makes IVSS hard to ...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['Surveillance image · blind quality assessment · deep neuralnetwork · visual saliency.']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['—Intelligent video surveillance system (IVSS)', 'saliency-based deep neural network (CNN)', 'blind quality assessmentI']\n",
      "\n",
      "================================================================================\n",
      "ABSTRACT SNIPPET:\n",
      "Overparameterized models fail to generalize well in the presence of data imbalance even when combined with traditional techniques for mitigating imbalances. This paper focuses on imbalanced classification datasets, in which a small subset of the population -- a minority -- may contain features that ...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['—fairness', 'benign overﬁtting', 'non-asymptoticI']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['Classiﬁcation · Overparameterized models · Data imbalances.']\n"
     ]
    }
   ],
   "source": [
    "example_predictions(model, tokenizer, test_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7298274,
     "sourceId": 11632226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
