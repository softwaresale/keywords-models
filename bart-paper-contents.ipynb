{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:28:44.873917Z",
     "iopub.status.busy": "2025-05-03T03:28:44.873701Z",
     "iopub.status.idle": "2025-05-03T03:29:09.002977Z",
     "shell.execute_reply": "2025-05-03T03:29:09.002185Z",
     "shell.execute_reply.started": "2025-05-03T03:28:44.873894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 03:28:55.282789: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746242935.467843      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746242935.518182      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, logging\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "import nltk\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.005323Z",
     "iopub.status.busy": "2025-05-03T03:29:09.004664Z",
     "iopub.status.idle": "2025-05-03T03:29:09.008933Z",
     "shell.execute_reply": "2025-05-03T03:29:09.008274Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.005299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.010528Z",
     "iopub.status.busy": "2025-05-03T03:29:09.009779Z",
     "iopub.status.idle": "2025-05-03T03:29:09.034262Z",
     "shell.execute_reply": "2025-05-03T03:29:09.033644Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.010502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.035077Z",
     "iopub.status.busy": "2025-05-03T03:29:09.034849Z",
     "iopub.status.idle": "2025-05-03T03:29:09.048013Z",
     "shell.execute_reply": "2025-05-03T03:29:09.047314Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.035059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KeywordDataset(Dataset):\n",
    "    def __init__(self, contents, keywords, tokenizer, max_length=1024, max_target_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.contents = contents\n",
    "        self.keywords = keywords\n",
    "        self.max_length = max_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.contents)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        content = self.contents[idx]\n",
    "        \n",
    "        # Convert keyword list to a comma-separated string\n",
    "        keyword_list = self.keywords[idx]\n",
    "        if isinstance(keyword_list, list):\n",
    "            keywords_text = \", \".join(keyword_list)\n",
    "        else:\n",
    "            keywords_text = keyword_list\n",
    "        \n",
    "        # Encode the inputs\n",
    "        inputs = self.tokenizer(\n",
    "            content, \n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Encode the targets\n",
    "        targets = self.tokenizer(\n",
    "            keywords_text,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs.input_ids.squeeze()\n",
    "        attention_mask = inputs.attention_mask.squeeze()\n",
    "        labels = targets.input_ids.squeeze()\n",
    "        \n",
    "        # Replace padding token id with -100 so it's ignored in loss calculation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.048992Z",
     "iopub.status.busy": "2025-05-03T03:29:09.048703Z",
     "iopub.status.idle": "2025-05-03T03:29:09.066195Z",
     "shell.execute_reply": "2025-05-03T03:29:09.065516Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.048963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"Clean and prepare the dataframes for training\"\"\"\n",
    "    # Clean abstracts\n",
    "    for df in [train_df, test_df]:\n",
    "        df['content'] = df['content'].str.replace(\"\\n\\n\", \" \", regex=False)\n",
    "        df['content'] = df['content'].str.replace(\"\\n\", \" \", regex=False)\n",
    "        df['content'] = df['content'].str.strip()\n",
    "        \n",
    "        # Ensure keywords are in list format\n",
    "        if not isinstance(df['keywords'].iloc[0], list):\n",
    "            df['keywords'] = df['keywords'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "            \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.067237Z",
     "iopub.status.busy": "2025-05-03T03:29:09.067019Z",
     "iopub.status.idle": "2025-05-03T03:29:09.081433Z",
     "shell.execute_reply": "2025-05-03T03:29:09.080831Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.067211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_training_data(train_df, test_df, model_name=\"facebook/bart-base\"):\n",
    "    \"\"\"Prepare datasets for BART training\"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    # Create datasets\n",
    "    train_dataset = KeywordDataset(\n",
    "        train_df['content'].tolist(),\n",
    "        train_df['keywords'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    eval_dataset = KeywordDataset(\n",
    "        test_df['content'].tolist(),\n",
    "        test_df['keywords'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    return train_dataset, eval_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.083925Z",
     "iopub.status.busy": "2025-05-03T03:29:09.083409Z",
     "iopub.status.idle": "2025-05-03T03:29:09.094786Z",
     "shell.execute_reply": "2025-05-03T03:29:09.094179Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.083909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_bart_model(train_dataset, eval_dataset, tokenizer, model_name=\"facebook/bart-base\"):\n",
    "    \"\"\"Train the BART model for keyword extraction\"\"\"\n",
    "    # Initialize model\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./bart-keyword-model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=5,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=128,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        gradient_accumulation_steps=2,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.095479Z",
     "iopub.status.busy": "2025-05-03T03:29:09.095258Z",
     "iopub.status.idle": "2025-05-03T03:29:09.110926Z",
     "shell.execute_reply": "2025-05-03T03:29:09.110177Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.095454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_fuzzy_map_score(true_keywords, predicted_keywords, fuzzy_threshold=80):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision with fuzzy matching\n",
    "    \n",
    "    Args:\n",
    "        true_keywords: List of ground truth keywords\n",
    "        predicted_keywords: List of predicted keywords in ranked order\n",
    "        fuzzy_threshold: Similarity threshold for fuzzy matching (0-100)\n",
    "        \n",
    "    Returns:\n",
    "        MAP score\n",
    "    \"\"\"\n",
    "    if not predicted_keywords or not true_keywords:\n",
    "        return 0.0\n",
    "    \n",
    "    # Track which true keywords have been matched\n",
    "    matched_keywords = set()\n",
    "    precision_sum = 0.0\n",
    "    num_hits = 0\n",
    "    \n",
    "    # Check each predicted keyword in order\n",
    "    for i, pred_kw in enumerate(predicted_keywords):\n",
    "        # Try to match with any unmatched true keyword\n",
    "        for true_kw in true_keywords:\n",
    "            if true_kw in matched_keywords:\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity\n",
    "            similarity = fuzz.ratio(pred_kw.lower(), true_kw.lower())\n",
    "            \n",
    "            # If it's a match, count it and mark as matched\n",
    "            if similarity >= fuzzy_threshold:\n",
    "                matched_keywords.add(true_kw)\n",
    "                num_hits += 1\n",
    "                precision_sum += num_hits / (i + 1)  # Precision at recall point i+1\n",
    "                break\n",
    "    \n",
    "    # Calculate final MAP\n",
    "    if num_hits > 0:\n",
    "        return precision_sum / min(len(true_keywords), len(predicted_keywords))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.111877Z",
     "iopub.status.busy": "2025-05-03T03:29:09.111692Z",
     "iopub.status.idle": "2025-05-03T03:29:09.128015Z",
     "shell.execute_reply": "2025-05-03T03:29:09.127340Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.111862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_semantic_map_score(true_keywords, predicted_keywords, semantic_model, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision with semantic similarity matching\n",
    "    \"\"\"\n",
    "    if not predicted_keywords or not true_keywords:\n",
    "        return 0.0\n",
    "    \n",
    "    # Track which true keywords have been matched\n",
    "    matched_keywords = set()\n",
    "    precision_sum = 0.0\n",
    "    num_hits = 0\n",
    "\n",
    "    semantic_model.show_progress_bar = False\n",
    "\n",
    "    #Converting predicted and true keywords to embeddings\n",
    "    true_embeddings = semantic_model.encode(true_keywords, convert_to_tensor=True, show_progress_bar=False)\n",
    "    predicted_embeddings = semantic_model.encode(predicted_keywords, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    for i, pred_emb in enumerate(predicted_embeddings):\n",
    "        unmatched_indices = [j for j, kw in enumerate(true_keywords) if kw not in matched_keywords]\n",
    "        if not unmatched_indices:\n",
    "            break\n",
    "        \n",
    "        unmatched_true_embs = true_embeddings[unmatched_indices]\n",
    "        cos_scores = util.cos_sim(pred_emb, unmatched_true_embs)[0] #calculating cosine similarity to all unmatched keywords\n",
    "        \n",
    "        # Find best match\n",
    "        best_idx = torch.argmax(cos_scores).item()\n",
    "        best_score = cos_scores[best_idx].item()\n",
    "\n",
    "        if best_score >= similarity_threshold:\n",
    "            matched_kw = true_keywords[unmatched_indices[best_idx]]\n",
    "            matched_keywords.add(matched_kw)\n",
    "            num_hits += 1\n",
    "            precision_sum += num_hits / (i + 1)\n",
    "    \n",
    "    if num_hits > 0:\n",
    "        return precision_sum / min(len(true_keywords), len(predicted_keywords))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.128924Z",
     "iopub.status.busy": "2025-05-03T03:29:09.128649Z",
     "iopub.status.idle": "2025-05-03T03:29:09.147529Z",
     "shell.execute_reply": "2025-05-03T03:29:09.146839Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.128907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_keyword_extraction(model, tokenizer, test_df, fuzzy_threshold=50, semantic_threshold=0.5, top_k=5):\n",
    "    \"\"\"Evaluate the model using MAP with fuzzy matching\"\"\"\n",
    "    model.eval()\n",
    "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    fuzzy_map_scores = []\n",
    "    semantic_map_scores = []\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    for i, row in notebook_tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        content = row['content']\n",
    "        true_keywords = row['keywords']\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(content, return_tensors=\"pt\", max_length=512, \n",
    "                          truncation=True).to(device)\n",
    "        \n",
    "        # Generate keywords\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids, \n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode and split the generated keywords\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted_keywords = [k.strip() for k in pred_text.split(',')][:top_k]\n",
    "        \n",
    "        # Calculate Fuzzy MAP score\n",
    "        fuzzy_map_score = calculate_fuzzy_map_score(true_keywords, predicted_keywords, fuzzy_threshold)\n",
    "        fuzzy_map_scores.append(fuzzy_map_score)\n",
    "\n",
    "        # Calculate Semantic MAP score\n",
    "        semantic_map_score = calculate_semantic_map_score(true_keywords, predicted_keywords, semantic_model, semantic_threshold)\n",
    "        semantic_map_scores.append(semantic_map_score)\n",
    "    \n",
    "     # Calculate average MAP scores\n",
    "    avg_fuzzy_map = sum(fuzzy_map_scores) / len(fuzzy_map_scores)\n",
    "    avg_semantic_map = sum(semantic_map_scores) / len(semantic_map_scores)\n",
    "    #print(f\"MAP@{top_k} Score with fuzzy threshold {fuzzy_threshold}: {avg_map:.4f}\")\n",
    "    \n",
    "    return avg_fuzzy_map, avg_semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.148291Z",
     "iopub.status.busy": "2025-05-03T03:29:09.148093Z",
     "iopub.status.idle": "2025-05-03T03:29:09.166384Z",
     "shell.execute_reply": "2025-05-03T03:29:09.165705Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.148276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def example_predictions(model, tokenizer, test_df, num_examples=5):\n",
    "    \"\"\"Show example predictions from the model\"\"\"\n",
    "    indices = np.random.choice(len(test_df), min(num_examples, len(test_df)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        content = test_df.iloc[idx]['content']\n",
    "        true_keywords = test_df.iloc[idx]['keywords']\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(content, return_tensors=\"pt\", max_length=512, \n",
    "                          truncation=True).to(device)\n",
    "        \n",
    "        # Generate keywords\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids, \n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode and split the generated keywords\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted_keywords = [k.strip() for k in pred_text.split(',')]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CONTENT SNIPPET:\")\n",
    "        print(content[:1000] + \"...\" if len(content) > 1000 else content)\n",
    "        print(\"\\nTRUE KEYWORDS:\")\n",
    "        print(true_keywords)\n",
    "        print(\"\\nPREDICTED KEYWORDS:\")\n",
    "        print(predicted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:09.167853Z",
     "iopub.status.busy": "2025-05-03T03:29:09.167148Z",
     "iopub.status.idle": "2025-05-03T03:29:20.471460Z",
     "shell.execute_reply": "2025-05-03T03:29:20.470576Z",
     "shell.execute_reply.started": "2025-05-03T03:29:09.167836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(\"data/training-data.ndjson\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = [json.loads(line) for line in f if line.strip()]  # skip blank lines\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df = train_df[['content', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:20.473021Z",
     "iopub.status.busy": "2025-05-03T03:29:20.472415Z",
     "iopub.status.idle": "2025-05-03T03:29:24.404130Z",
     "shell.execute_reply": "2025-05-03T03:29:24.403228Z",
     "shell.execute_reply.started": "2025-05-03T03:29:20.472997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with open(\"data/test-data.ndjson\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = [json.loads(line) for line in f if line.strip()]  # skip blank lines\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df = test_df[['content', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T03:29:24.405794Z",
     "iopub.status.busy": "2025-05-03T03:29:24.405086Z",
     "iopub.status.idle": "2025-05-03T07:53:46.323898Z",
     "shell.execute_reply": "2025-05-03T07:53:46.323142Z",
     "shell.execute_reply.started": "2025-05-03T03:29:24.405771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training loop for BART model trained on paper contents\n",
      "Training on 16000 examples, testing on 4000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a417e2953b4bc39f26f415b40c0df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44ba8d1761a46e5bd838558937f7b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0dbddbc7ce4441abd5897045520e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f17ce8288d4bdd91566f9633685e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30679391bd048deb79d270cb942cf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 4:01:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.773200</td>\n",
       "      <td>2.426451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.555300</td>\n",
       "      <td>2.334561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.375500</td>\n",
       "      <td>2.292436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.302100</td>\n",
       "      <td>2.273286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.251900</td>\n",
       "      <td>2.258675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07db6c49aa44e458af5e722ca7ad25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aac49f450b0453fbff4fa55c95d498a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4308700516849a6972b2ceac959f223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8e873a283144c5895d3ad4710c9469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77808b1ac2064102913c803b73369a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd87210fd982487cae4ade98c54ef339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192bb13bddfd4382a5ceb6a055dc1a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34559fd70cc949acab392519a603b7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2514071b3a41a4867ad60420fe1ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e9d270b7654e4d92e6e11cf6dbe5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e730ee0cafe4110bf281a0a840e2b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd63dbe028941629e9b7f42a4b849e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Matching MAP score for model trained on paper contents: 0.37626590277777963\n",
      "Semantic Matching MAP score for model trained on paper contents: 0.5633136805555579\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "train_df, test_df = preprocess_data(train_df, test_df)\n",
    "print(f\"Beginning training loop for BART model trained on paper contents\")\n",
    "print(f\"Training on {len(train_df)} examples, testing on {len(test_df)} examples\")\n",
    "    \n",
    "train_dataset, eval_dataset, tokenizer = prepare_training_data(train_df, test_df)\n",
    "    \n",
    "#Model Training\n",
    "model, trainer = train_bart_model(train_dataset, eval_dataset, tokenizer)\n",
    "    \n",
    "#Model Evaluation\n",
    "fuzzy_map_score, semantic_map_score = evaluate_keyword_extraction(model, tokenizer, test_df)\n",
    "print(f\"Fuzzy Matching MAP score for model trained on paper contents: {fuzzy_map_score}\")\n",
    "print(f\"Semantic Matching MAP score for model trained on paper contents: {semantic_map_score}\")\n",
    "    \n",
    "# Step 5: Show example predictions\n",
    "#example_predictions(model, tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T07:53:46.325091Z",
     "iopub.status.busy": "2025-05-03T07:53:46.324831Z",
     "iopub.status.idle": "2025-05-03T08:16:12.204794Z",
     "shell.execute_reply": "2025-05-03T08:16:12.203975Z",
     "shell.execute_reply.started": "2025-05-03T07:53:46.325072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55090e509089468fa9861d53fa52b28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Matching MAP score for model trained on paper contents: 0.38626590277777963\n",
      "Semantic Matching MAP score for model trained on paper contents: 0.4304836111111122\n"
     ]
    }
   ],
   "source": [
    "fuzzy_map_score, semantic_map_score = evaluate_keyword_extraction(model, tokenizer, test_df, semantic_threshold=0.6)\n",
    "print(f\"Fuzzy Matching MAP score for model trained on paper contents: {fuzzy_map_score}\")\n",
    "print(f\"Semantic Matching MAP score for model trained on paper contents: {semantic_map_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:04.189330Z",
     "iopub.status.busy": "2025-05-03T08:21:04.188599Z",
     "iopub.status.idle": "2025-05-03T08:21:05.283246Z",
     "shell.execute_reply": "2025-05-03T08:21:05.282587Z",
     "shell.execute_reply.started": "2025-05-03T08:21:04.189305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTENT SNIPPET:\n",
      "1  Introduction  With the groundbreaking discovery of simple and complex cells as well as their receptive field  arrangements, Hubel and Wiesel (1959, 1962) not only laid the foundation for decades of subsequent  findings in visual neuroscience but also ignited an idea in another field emergent at that time – computer  vision.  The  Neocognitron  was  introduced  by  Fukushima  and  Miyake  (1982),  who  translated  the  aforementioned findings to one of the earliest multi-layer artificial neural networks and thereby set the  cornerstone for image and pattern understanding in deep learning. Nearly half a century later, computer  and biological vision research cannot be imagined without the Neocognitron’s well-known successors,  commonly referred to as Deep Convolutional Neural Networks (DCNNs; LeCun et al., 2015), the state- of-the-art  models  of  biological  processes  such  as  object  recognition.  View-invariant  core  object  recognition, that is “the ability to rapidly recognize...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['vision', 'attention', 'brain', 'deep neural networks', 'eye tracking', 'saliency map', 'object recognition', 'face detection']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['Neocognitron', 'Deep Convolutional Neural Networks', 'Computer Vision', 'Object Recognition']\n",
      "\n",
      "================================================================================\n",
      "CONTENT SNIPPET:\n",
      "1 Introduction Image style transfer aims to automatically transfer the artistic style from a source style image to a given content one, and has been studied for a long time in the computer vision community. Conventionally, image style transfer is generally cast as the problem of non-photorealistic rendering in the domain of computer graphics. Inspired by the success of deep learning [9,41,8,55,10], Gatys et al. [11] pioneer the paradigm that leverages the feature activations from deeparXiv:2207.11681v2  [cs.CV]  13 Feb 2023 2 Y. Jing et al.2 Y. Jing et al. Content+Style Huang et al.[14] An et al.[1] Li et al. [30] Chen et al. [6] Sheng et al. [42] Ours Fig. 1. Existing parametric [14, 1, 30] and non-parametric [6, 42] NST methods either barely transfer the global style appearance to the target [6], or produce distorted lo- cal style patterns [14, 1, 30] and undesired artifacts [42]. By contrast, the proposed GNN-based semi-parametric approach achieves superior stylization performance i...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['Neural style transfer · Graph neural networks · Attention-based message passing']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['Neural style transfer · Artiﬁcial intelligence · Image style transfer']\n",
      "\n",
      "================================================================================\n",
      "CONTENT SNIPPET:\n",
      "1. Introduction Speech compression aims to reduce the bitrate required to rep- resent a speech signal. In classical coding methods [1–7], all processing was based on knowledge of human experts only. Re- cent advances in speech coding follow progress in speech syn- thesis [8–10] by replacing the decoder [11–13] as well as the quantizer [14] with a machine-learning (ML) based model that signiﬁcantly improves the coding quality. More recently, end- to-end coding schemes have been developed [15, 16] that em- ploy an autoencoder structure with quantization in the bottle- neck (latent space). With SoundStream [16], this autoencoding structure, in the form of a VQ-VAE [17], was further combined with the learned distortion measures from generative adversarial networks (GANs) [18]. While [16] represents the current state- of-the-art above 3 kbps, its relative effectiveness deteriorates at lower rates. Indications exist [19] that lengthening the effec- tive receptive ﬁeld of the encoder may impr...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "[': speech coding', 'Transformers', 'self-supervisedlearning', 'generative adversarial nets']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "[': speech compression', 'generative adversarial networks', 'deep learning', 'neural networks1']\n",
      "\n",
      "================================================================================\n",
      "CONTENT SNIPPET:\n",
      "1 Introduction With the rapid development of computer vision technology and the growing de- mand for security, recent years have witnessed the increasing popularity of the intelligent video surveillance system (IVSS), which can automatically analyze and understand the content of surveillance videos and thus greatly reduce the burden of the manual labour. Speciﬁcally, IVSS involves analyzing the videos using algorithms that detect, track, and recognize objects of interest [1]. How- ever, the surveillance images (SIs) may suﬀer diﬀerent degrees of degradationsarXiv:2206.04318v1  [cs.MM]  9 Jun 2022 2 W. Lu et al. (a) (b) Fig. 1. Examples of the SIs with low image quality, which are from the SI quality database (SIQD) [2]. The red rectangle region is the blurred face and the green rectangle region is the clear background. in quality due to the artifacts introduced to the SI acquisition and transmission process [2,3,4,5], such as poor lighting conditions, low compression bit rates, etc. Th...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['Surveillance image · blind quality assessment · deep neuralnetwork · visual saliency.']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['Intelligent video surveillance system · Objective image quality assessment']\n",
      "\n",
      "================================================================================\n",
      "CONTENT SNIPPET:\n",
      "I. INTRODUCTION The overparameterization trend in machine learning (i.e., training large models whose size far exceeds that of the training set) has been popularized following repeated empirical observations that large models achieve state-of-the-art accuracy despite being able to perfectly ﬁt / interpolate the training data [1]–[3]. These surprising empirical ﬁndings have precipitated a surge of research activity towards developing a theory of (so- called) harmless interpolation or benign overﬁtting that seeks explanations and better understanding of the interplay between large models, (gradient-based) optimization and data, e.g. [4]– [7]. Such a theory is still at its infancy. Yet, remarkable progress has contributed to reinforcing the modern wisdom: “large models train better and training longer improves accuracy.” On the other hand, the inherent ability of overparameterized models to perfectly ﬁt any training objective hides risks. Speciﬁcally, this becomes relevant when learning f...\n",
      "\n",
      "TRUE KEYWORDS:\n",
      "['—fairness', 'benign overﬁtting', 'non-asymptoticI']\n",
      "\n",
      "PREDICTED KEYWORDS:\n",
      "['—overparameterization', 'weighted cross-entropy', 'classiﬁcationI']\n"
     ]
    }
   ],
   "source": [
    "example_predictions(model, tokenizer, test_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7298274,
     "sourceId": 11632226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
